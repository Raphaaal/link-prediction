{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"encoding":"# -*- coding: utf-8 -*-","formats":"py:light,ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.4"},"colab":{"name":"REAL_03_Train_Test_Split.ipynb","provenance":[],"collapsed_sections":["ZoaqhgJQC_fC","E7kxaXsfldsD"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZoaqhgJQC_fC"},"source":["# Setup\n","\n","Install the necessary libraries in your Colab notebook environment and connect to your hosted Neo4J Sandbox."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmHymBmsVYqx","executionInfo":{"status":"ok","timestamp":1623140424126,"user_tz":-120,"elapsed":43846,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"69f973c7-443a-4f3e-aa0e-586cec66899b"},"source":["!pip install neo4j pyspark matplotlib"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting neo4j\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/22/9b6d28613e8a564b9e82cf3b871b6df1f58a99cf5ac8a100439f9e895b5f/neo4j-4.3.1.tar.gz (74kB)\n","\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n","\u001b[?25hCollecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n","\u001b[K     |████████████████████████████████| 212.4MB 70kB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from neo4j) (2018.9)\n","Collecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 36.6MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n","Building wheels for collected packages: neo4j, pyspark\n","  Building wheel for neo4j (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for neo4j: filename=neo4j-4.3.1-cp37-none-any.whl size=99332 sha256=cfb0fbf494eda198a8aadb95dfd55bf87cd98752b884262dc7d9e75581143b81\n","  Stored in directory: /root/.cache/pip/wheels/23/13/72/0cc2405898bd9a7baef6512df3abf83873da9ba48c04acc818\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=689cd08326259905576f823bbde27f15015336e52114048cda1bff30be363cff\n","  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n","Successfully built neo4j pyspark\n","Installing collected packages: neo4j, py4j, pyspark\n","Successfully installed neo4j-4.3.1 py4j-0.10.9 pyspark-3.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fhmJ1QEdC-4e"},"source":["ip = \"54.172.14.140\"\n","bolt_port = \"7687\"\n","username = \"neo4j\"\n","password = \"rifle-sponsor-beliefs\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RI-e7X2UuOgR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623140424143,"user_tz":-120,"elapsed":77,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"68cea97b-9d05-4ed0-c28b-20c167a580c3"},"source":["from neo4j import GraphDatabase\n","\n","driver = GraphDatabase.driver(\"bolt://\" + ip + \":\" + bolt_port, auth=(username, password))\n","\n","print(driver.address) # your-sandbox-ip:your-sandbox-bolt-port"],"execution_count":null,"outputs":[{"output_type":"stream","text":["54.172.14.140:7687\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j_3fld9VKPs4"},"source":["import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n","from neo4j import unit_of_work\n","\n","plt.style.use('fivethirtyeight')\n","pd.set_option('display.float_format', lambda x: '%.3f' % x)\n","\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5aOcDSFldr-"},"source":["# Train / test split\n","\n","We need to produce a train and test datasets on which we can build, and then evaluate a model. This binary classifier will predict wether the two nodes composing the pair shoulde be linked (1) or not (0).\n","\n","This classifier will be trained on a dataset composed of nodes pairs and tested on an isolated dataset of nodes pairs. We need to be careful when splitting the data, because pairs of nodes in our training set should not be connected to those in the test set.\n","\n","In order to build the features for each pair of nodes in the training set, we will compute measures that should not contain information from the test set (and vice versa). **The solution is to split our co-authorship graph into two sub graphs of similar structure**. \n","\n","We will use the year of first collaboration to split our initial graph in two."]},{"cell_type":"markdown","metadata":{"id":"RrynYYXMldsA"},"source":["## Year split\n","\n","We can create train and test graphs by splitting the data on a particular year. Now we need to figure out what year that should be. Let's have a look at the distribution of the first year that co-authors collaborated."]},{"cell_type":"markdown","metadata":{"id":"sNzeR6rCKpxO"},"source":["- Count the number of CO_AUTHOR relationships by year of first collaboration"]},{"cell_type":"code","metadata":{"id":"jznO-yC3LVXC"},"source":["query = \"\"\"\n","  MATCH ... \n","  WITH ... AS year, ... AS count\n","  ORDER BY ...\n","  RETURN ... AS year, count\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TaJRe28Ibup","cellView":"form"},"source":["#@title Hint\n","\n","query = \"\"\"\n","  MATCH ()-[...]->() // Use a \"r\" naming to handle the CO_AUTHOR relationship\n","  WITH r.year AS year, ... AS count\n","  ORDER BY year\n","  RETURN toString(year) AS year, count\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"1BOhn-NZMuvs"},"source":["#@title Solution\n","\n","query = \"\"\"\n","MATCH ()-[r:CO_AUTHOR]->()\n","WITH r.year AS year, COUNT(*) AS count\n","ORDER BY year\n","RETURN toString(year) AS year, count\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtLlbAjWM7SA"},"source":["- Save the results of your counting in a Spark DataFrame by completing the cell below"]},{"cell_type":"code","metadata":{"id":"8O7cEFmSQAvl","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"error","timestamp":1623140428277,"user_tz":-120,"elapsed":209,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"0c04e52f-0d5b-4921-d9dc-a8844586e3b4"},"source":["with driver.session() as session:\n","    result = session.run(query)\n","    # Convert the result to a list of dictionaries\n","    result_dict = [... for ... in ...]\n","    # Create a Spark DataFrame from this list\n","    co_authorships_by_year = ..."],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-04f875542749>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    result_dict = [... for ... in ...]\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to Ellipsis\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","id":"TYOuTwT_P3nW"},"source":["#@title Solution\n","\n","with driver.session() as session:\n","    result = session.run(query)\n","    # Convert the result to a list of dictionaries\n","    result_dict = [dict(record) for record in result]\n","    # Create DataFrame from this list\n","    co_authorships_by_year = spark.createDataFrame(result_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbCuRh7zPtdZ"},"source":["- Plot the distribution as a bar graph "]},{"cell_type":"code","metadata":{"id":"fHNuRcNmldsA"},"source":["# We convert to a Pandas DataFrame because of better integration with matplotlib   \n","co_authorships_by_year = co_authorships_by_year.toPandas() \n","\n","# Plot\n","ax = co_authorships_by_year.plot(kind='bar', x='year', y='count', legend=None, figsize=(15,8))\n","ax.xaxis.set_label_text(\"Nb of CO_AUTHOR links by year of first collaboration\")\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"azLvZUikldsA"},"source":["It looks like 2006 would act as a good year on which to split the data. We'll take all the co-authorships from 2005 and earlier as our train graph, and everything from 2006 onwards as the test graph.\n","\n","Let's create explicit *CO_AUTHOR_EARLY* and *CO_AUTHOR_LATE* relationships in our graph based on that year. We will then use them to distinguish train and test sets."]},{"cell_type":"markdown","metadata":{"id":"WRdOYFrQQnxl"},"source":[" - Create the *CO_AUTHOR_EARLY* and *CO_AUTHOR_LATE* relationships"]},{"cell_type":"code","metadata":{"id":"JtN6mMEdQnFT"},"source":["query_early = \"\"\"\n","  MATCH (...)-[...]->(...)\n","  WHERE ...\n","  MERGE (...)-[:CO_AUTHOR_EARLY {year: r.year}]-(...)\n","\"\"\"\n","\n","query_late = \"\"\"\n","  MATCH (...)-[...]->(...)\n","  WHERE ...\n","  MERGE (...)-[:CO_AUTHOR_LATE {year: r.year}]-(...)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41d5bxMmQnMr","cellView":"form"},"source":["#@title Solution\n","\n","query_early = \"\"\"\n","  MATCH (a)-[r:CO_AUTHOR]->(b)\n","  WHERE r.year < 2006\n","  MERGE (a)-[:CO_AUTHOR_EARLY {year: r.year}]-(b)\n","\"\"\"\n","\n","query_late = \"\"\"\n","  MATCH (a)-[r:CO_AUTHOR]->(b)\n","  WHERE r.year >= 2006\n","  MERGE (a)-[:CO_AUTHOR_LATE {year: r.year}]-(b)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YYv5OWcQldsA"},"source":["with driver.session() as session:\n","    session.run(query_early)\n","\n","with driver.session() as session:\n","    session.run(query_late)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNkhvKdcldsA"},"source":["- Count how many *CO_AUTHOR* relationships we have in each of these sub graphs and store these count in  Spark Dataframe.\n","\n","Hint: use the *CO_AUTHOR_EARLY* and *CO_AUTHOR_LATE* relationships that we have just created."]},{"cell_type":"code","metadata":{"id":"pDljASANTckG"},"source":["query = \"\"\"\n","  MATCH ...\n","  WHERE ...\n","  RETURN TYPE(r), COUNT(*) AS count\n","\"\"\"\n","\n","with driver.session() as session:\n","    result = session.run(query)\n","    early_late_count = ...\n","\n","early_late_count.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCyjg06VldsA","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1623140458672,"user_tz":-120,"elapsed":3810,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"ee4e839d-71ce-426f-b407-de319404615f"},"source":["#@title Solution\n","\n","query = \"\"\"\n","  MATCH ()-[r]->()\n","  WHERE TYPE(r) = \"CO_AUTHOR_EARLY\" OR TYPE(r) = \"CO_AUTHOR_LATE\"\n","  RETURN TYPE(r), COUNT(*) AS count\n","\"\"\"\n","\n","with driver.session() as session:\n","    result = session.run(query)\n","    early_late_count = spark.createDataFrame([dict(record) for record in result])\n","\n","early_late_count.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------------+-----+\n","|        TYPE(r)|count|\n","+---------------+-----+\n","| CO_AUTHOR_LATE|74128|\n","|CO_AUTHOR_EARLY|81096|\n","+---------------+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E7kxaXsfldsD"},"source":["## Class imbalance considerations\n","\n","Using 2006 as the split year, we have a split of 52% - 48% split of *CO_AUTHOR* relationships. We will use this split to separate our **positive examples** (i.e. pairs of nodes that we will label with 1) in train and test set.\n","\n","We now need to split our **negative examples** (i.e. pairs of nodes that we will label with 0, because no *CO_AUTHOR* link exist between them) between train and test sets.\n","\n","The simplest approach would be to use all pair of nodes within each sub graph that don’t have a relationship. The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do. If we use all of these negative examples in our training set we will have a massive class imbalance. "]},{"cell_type":"markdown","metadata":{"id":"rTriIChAWRjq"},"source":["- Why would it be bad to use such an imbalanced training set in this binary classification problem?"]},{"cell_type":"code","metadata":{"id":"zxfc8LGNWQz8","cellView":"form"},"source":["#@title Solution\n","\n","# A model trained using data that is this imbalanced will achieve very high accuracy simply by predicting 0 all the time (i.e. that any pair of nodes do not have a relationship between them)."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9YFmxwGjldsD"},"source":["\n","We need to reduce the number of negative examples to produced a training dataset with balanced classes. \n","\n","To do so, we will:\n","- use pairs of nodes that are two to three of hops away from each other\n","- further down sample the negative examples if necessary"]},{"cell_type":"markdown","metadata":{"id":"ZRwmrikioJHK"},"source":["## Create the training set pairs\n"]},{"cell_type":"markdown","metadata":{"id":"x3i_H5haYLGl"},"source":["- Extract the **positive examples** for the training set in a Spark DataFrame. That is the pairs of nodes that are linked by a *CO_AUTHOR_EARLY* relationship."]},{"cell_type":"code","metadata":{"id":"eL3FGUb3Y6DQ"},"source":["query = \"\"\"\n","  MATCH (...)-[...]->(...)\n","  RETURN ... AS node1, ... AS node2, ... AS label // Use node ID \n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdc73RmYZq3P","cellView":"form"},"source":["#@title Solution\n","\n","query = \"\"\"\n","  MATCH (author:Author)-[:CO_AUTHOR_EARLY]->(other:Author) // Directional link to avoid duplicates\n","  RETURN ID(author) AS node1, ID(other) AS node2, 1 AS label\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvFiu397YhRE"},"source":["with driver.session() as session:\n","    result = session.run(query)\n","    train_existing_links = spark.createDataFrame([dict(record) for record in result])\n","    train_existing_links = train_existing_links.dropDuplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ql3Q6pkQZ8F6"},"source":["- Extract the **negative examples** for the training set in a Spark DataFrame. That is the pairs of *Author* nodes in the *EARLY* sub graph that are two to three hops away and that did not collaborate together yet."]},{"cell_type":"code","metadata":{"id":"7MdISIUoacUp"},"source":["query = \"\"\"\n","  MATCH (author:Author)\n","  WHERE (author)-[...]->()\n","  MATCH (author)-[...*...]->(...) // Two to three hops away\n","  WHERE NOT((author)-[...]-(...)) // Did not collaborate together\n","  RETURN ... AS node1, ... AS node2, ... AS label // Use node ID \n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PR8u5Z1aUBW","cellView":"form"},"source":["#@title Solution\n","\n","query = \"\"\"\n","  MATCH (author:Author)\n","  WHERE (author)-[:CO_AUTHOR_EARLY]->()\n","  MATCH (author)-[:CO_AUTHOR_EARLY*2..3]->(other)\n","  WHERE NOT((author)-[:CO_AUTHOR_EARLY]-(other))\n","  RETURN ID(author) AS node1, ID(other) AS node2, 0 AS label\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmXsm64fldsD"},"source":["with driver.session() as session:\n","    result = session.run(query)\n","    train_missing_links = spark.createDataFrame([dict(record) for record in result])    \n","    train_missing_links = train_missing_links.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isVVPHeIRquI"},"source":["We add the positive pairs to the negative ones in order to create our training set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcCP2d9ZRwZ9","executionInfo":{"status":"ok","timestamp":1623140502790,"user_tz":-120,"elapsed":8955,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"9e4ee8b0-539a-477e-8c73-54d5e1f91543"},"source":["training_df = train_missing_links.union(train_existing_links)\n","print('Observations in training set: ', training_df.count())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Observations in training set:  185382\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TWBhn30vbWhl"},"source":["- Count the number of positive and negative pairs in the training set to evaluate class imbalance.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peAYj2zildsD","executionInfo":{"status":"ok","timestamp":1623082976736,"user_tz":-120,"elapsed":210,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"01263d26-0e3f-4d78-b750-15338b2691c9"},"source":["print('Train set class imbalance:')\n","# training_df. ... .show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train set class imbalance:\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"fWztw9NQRYDc","executionInfo":{"status":"ok","timestamp":1623140514481,"user_tz":-120,"elapsed":11807,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"d669214b-64ff-4a43-9e8c-051c191c6c87"},"source":["#@title Solution\n","print('Train set class imbalance:')\n","training_df.groupBy(F.col('label')).count().show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train set class imbalance:\n","+-----+------+\n","|label| count|\n","+-----+------+\n","|    0|104286|\n","|    1| 81096|\n","+-----+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rDYN8TH0bqkx"},"source":["- Randomly downsample the **negative examples** to get a balanced training dataset."]},{"cell_type":"code","metadata":{"id":"ZrjvuD22TnUK"},"source":["### Isolate and count the negative examples\n","\n","# train_df_class_0 = training_df. ...\n","# train_count_class_0 = train_df_class_0. ...\n","\n","### Isolate and count the positive examples\n","\n","# train_df_class_1 = training_df. ...\n","# train_count_class_1 = train_df_class_1. ...\n","\n","### Compute the proportion of positive examples\n","\n","# fraction = ... / ...\n","\n","### Sample the negative examples\n","### Hint use pyspark.sql.DataFrame.sample function \n","\n","# train_df_class_0_under = train_df_class_0. ...\n","\n","### Add these sample to the positive examples to get a balanced training set\n","df_train_under = train_df_class_0_under.union(train_df_class_1)\n","\n","# Show class imbalance now\n","print('Train set class imbalance after downsampling:')\n","# df_train_under. ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRrZt9xwmb-i","cellView":"form","executionInfo":{"status":"ok","timestamp":1623140537644,"user_tz":-120,"elapsed":23197,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"2ab29fc4-2a84-44e3-ef15-0d36e7745672"},"source":["#@title Solution\n","\n","# Isolate and count the negative examples\n","train_df_class_0 = training_df.filter(F.col('label') == 0) \n","train_count_class_0 = train_df_class_0.count()\n","\n","# Isolate and count the positive examples\n","train_df_class_1 = training_df.filter(F.col('label') == 1)\n","train_count_class_1 = train_df_class_1.count()\n","\n","# Compute the proportion of positive examples\n","fraction = train_count_class_1/train_count_class_0\n","\n","# Sample the negative examples\n","# Hint use pyspark.sql.DataFrame.sample function \n","\n","train_df_class_0_under = train_df_class_0.sample(withReplacement=False, fraction=fraction, seed=42) # Note: Spark does not guarantee the fraction to be exactly respected\n","\n","# Add these sample to the positive examples to get a balanced training set\n","df_train_under = train_df_class_0_under.union(train_df_class_1)\n","\n","# Show class imbalance now\n","print('Train set class imbalance after downsampling:')\n","df_train_under.groupBy(F.col('label')).count().show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train set class imbalance after downsampling:\n","+-----+-----+\n","|label|count|\n","+-----+-----+\n","|    0|81138|\n","|    1|81096|\n","+-----+-----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D2c6VxURoNLk"},"source":["## Create the test set pairs\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jVdiDHpoldsD"},"source":["Let's now do the same thing for the test set."]},{"cell_type":"markdown","metadata":{"id":"XWMmbbzQU-7n"},"source":["In a single session:\n","\n","- Extract the **positive examples** for the test set in a Spark DataFrame. That is the pairs of nodes that are linked by a *CO_AUTHOR_LATE* relationship.\n","- Extract the **negative examples** for the test set in a Spark DataFrame. That is the pairs of *Author* nodes in the *LATE* sub graph that are two to three hops away and that did not collaborate together yet."]},{"cell_type":"code","metadata":{"id":"nzgI4Fm2V2ku"},"source":["with driver.session() as session:\n","    result = session.run(\n","        \"\"\"\n","          MATCH (...)-[...]->(...)\n","          RETURN ... AS node1, ... AS node2, ... AS label\n","        \"\"\")\n","    test_existing_links = spark.createDataFrame([dict(record) for record in result])\n","    test_existing_links = test_existing_links.dropDuplicates()\n","\n","    result = session.run(\n","        \"\"\"\n","          MATCH (author:Author)\n","          WHERE (author)-[...]->()\n","          MATCH (author)-[... * ...]->(...)\n","          WHERE NOT((author)-[...]-(...))\n","          RETURN ... AS node1, ... AS node2, ... AS label\n","        \"\"\")\n","    test_missing_links = spark.createDataFrame([dict(record) for record in result])    \n","    test_missing_links = test_missing_links.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrxzqoSuldsD","cellView":"form"},"source":["#@title Solution\n","\n","with driver.session() as session:\n","    result = session.run(\n","        \"\"\"\n","          MATCH (author:Author)-[:CO_AUTHOR_LATE]->(other:Author)\n","          RETURN ID(author) AS node1, ID(other) AS node2, 1 AS label\n","        \"\"\")\n","    test_existing_links = spark.createDataFrame([dict(record) for record in result])\n","    test_existing_links = test_existing_links.dropDuplicates()\n","\n","    result = session.run(\n","        \"\"\"\n","          MATCH (author:Author)\n","          WHERE (author)-[:CO_AUTHOR_LATE]->()\n","          MATCH (author)-[:CO_AUTHOR_LATE*2..3]->(other)\n","          WHERE NOT((author)-[:CO_AUTHOR_LATE]-(other))\n","          RETURN ID(author) AS node1, ID(other) AS node2, 0 AS label\n","        \"\"\")\n","    test_missing_links = spark.createDataFrame([dict(record) for record in result])    \n","    test_missing_links = test_missing_links.drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_YWtdLxXHcg"},"source":["We add the positive pairs to the negative ones in order to create our testing set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Go1LOmlgldsD","executionInfo":{"status":"ok","timestamp":1623140579776,"user_tz":-120,"elapsed":4003,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"00a1eb4e-66bf-4af1-db39-ea3104945af9"},"source":["testing_df = test_missing_links.union(test_existing_links)\n","print('Observations in test set: ', testing_df.count())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Observations in test set:  194790\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_jcoWTrUYQnK"},"source":["- Count the number of positive and negative pairs in the test set to evaluate class imbalance.\n"]},{"cell_type":"code","metadata":{"id":"UJfS38ApYfja"},"source":["print('Test set class imbalance after downsampling:')\n","# testing_df. ... .show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"xHHI6qPhYb1C","executionInfo":{"status":"ok","timestamp":1623084656991,"user_tz":-120,"elapsed":9454,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"d43b77c5-9360-43ac-cfad-789202745832"},"source":["#@title Solution\n","\n","print('Test set class imbalance:')\n","testing_df.groupBy(F.col('label')).count().show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test set class imbalance:\n","+-----+------+\n","|label| count|\n","+-----+------+\n","|    0|120662|\n","|    1| 74128|\n","+-----+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mcF6uqtqdDOY"},"source":["In the test set, we do not want a balanced dataset because we need to model the real structure of observations on which this link prediction model could be tested.\n","\n","This is why we want a density close to: \n","\n","*(# CO_AUTHOR relationships) / (# pairs of Authors without CO_AUTHOR relationship)*.\n","\n","We know that:\n","\n","*# pairs of Authors without CO_AUTHOR relationship = (# Authors)² - (# CO_AUTHOR relationships) - (# Authors)*.\n","\n","So let's calculate the target density needed in our test set."]},{"cell_type":"code","metadata":{"id":"sqhwRY-2wRmL"},"source":["with driver.session() as session:\n","\n","    # Compute the number of Authors\n","    result = session.run(\n","        \"\"\"\n","          MATCH (...)\n","          RETURN ... as nb_authors\n","       \"\"\")\n","    nb_authors = [dict(record) for record in result][0]['nb_authors']\n","\n","    # Compute the number of CO_AUTHOR relationships\n","    result = session.run(\n","        \"\"\"\n","          MATCH (...)-[...]-(...)\n","          RETURN ... as nb_rels\n","       \"\"\")\n","    nb_co_author_relationships = [dict(record) for record in result][0]['nb_rels']\n","\n","    # Compute the target density for the test set\n","    nb_negative_pairs = (nb_authors * nb_authors) - nb_co_author_relationships - nb_authors\n","    density = nb_co_author_relationships / nb_negative_pairs\n","    print(\"Target density in test set: \", density)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"ILYZhkeYhPkD","executionInfo":{"status":"ok","timestamp":1623140580647,"user_tz":-120,"elapsed":899,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"c4ec8422-0170-404b-e7d1-b33858ea7174"},"source":["#@title Solution\n","\n","with driver.session() as session:\n","\n","    # Compute the number of Authors\n","    result = session.run(\n","        \"\"\"\n","          MATCH (a1:Author)\n","          RETURN COUNT(DISTINCT a1) as nb_authors\n","       \"\"\")\n","    nb_authors = [dict(record) for record in result][0]['nb_authors']\n","\n","    # Compute the number of CO_AUTHOR relationships\n","    result = session.run(\n","        \"\"\"\n","          MATCH (a1:Author)-[r:CO_AUTHOR]-(a2:Author)\n","          RETURN COUNT(DISTINCT r) as nb_rels\n","       \"\"\")\n","    nb_co_author_relationships = [dict(record) for record in result][0]['nb_rels']\n","\n","    # Compute the target density for the test set\n","    nb_negative_pairs = (nb_authors*nb_authors) - nb_co_author_relationships - nb_authors\n","    density = nb_co_author_relationships / nb_negative_pairs\n","    print(\"Target density in test set: \", density)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Target density in test set:  2.4074343933981664e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jGKDFswsqSR7"},"source":["Considering the number of negative pairs that we have extracted, respecting this density would produce only a couple of positive examples in the test set. This is a bit extreme for our future classifier evaluation. \n","\n","This is why we will use an arbitrary density of 0.01 for the sake of the illustration.\n","\n","Therefore, we need to downsample our positive class for the test set to reflect this density of 0.01."]},{"cell_type":"code","metadata":{"id":"Uw00P4WWxZVI"},"source":["approx_density = 0.01\n","\n","### Isolate and count the negative examples\n","\n","# test_df_class_0 = testing_df. ...\n","# test_count_class_0 = test_df_class_0. ...\n","\n","### Isolate and count the positive examples\n","\n","# test_df_class_1 = testing_df. ...\n","# test_count_class_1 = test_df_class_1. ...\n","\n","### Sample the positive examples\n","### Hint use pyspark.sql.DataFrame.sample function \n","\n","# test_df_class_1_under = test_df_class_1. ... \n","\n","### Add these sample to the negative examples to get a realistic test set\n","df_test_under = test_df_class_1_under.union(test_df_class_0)\n","\n","# Show class imblance now\n","print('Test set class imbalance after downsampling:')\n","df_test_under.groupBy(F.col('label')).count().show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIjQQfbBXdPW","executionInfo":{"status":"ok","timestamp":1623141232476,"user_tz":-120,"elapsed":16707,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"f93baa78-ac4b-41db-bea1-2b38c74d476c"},"source":["#@title Solution\n","\n","approx_density = 0.01\n","\n","### Isolate and count the negative examples\n","\n","test_df_class_0 = testing_df.filter(F.col('label') == 0)\n","test_count_class_0 = test_df_class_0.count()\n","\n","### Isolate and count the positive examples\n","\n","test_df_class_1 = testing_df.filter(F.col('label') == 1)\n","test_count_class_1 = test_df_class_1.count()\n","\n","### Sample the positive examples\n","test_df_class_1_under = test_df_class_1.sample(withReplacement=False, fraction=approx_density, seed=42) # Note: Spark does not guarantee the fraction to be exactly respected\n","\n","### Add these sample to the negative examples to get a realistic test set\n","df_test_under = test_df_class_1_under.union(test_df_class_0)\n","\n","# Show class imblance now\n","print('Test set class imbalance after downsampling:')\n","df_test_under.groupBy(F.col('label')).count().show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test set class imbalance after downsampling:\n","+-----+------+\n","|label| count|\n","+-----+------+\n","|    0|120662|\n","|    1|   718|\n","+-----+------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JF8no09gooKF"},"source":["# Save our train and test pairs DataFrames to CSV"]},{"cell_type":"markdown","metadata":{"id":"1rXpnmmpldsD"},"source":["Let's have a look at the contents of our train and test DataFrames before saving them on Google Drive."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L2pq1BXCldsD","executionInfo":{"status":"ok","timestamp":1623141243807,"user_tz":-120,"elapsed":6662,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"fa704f5a-342b-4348-e9ee-be2f717289cf"},"source":["df_train_under.filter(F.col('label') == 0).show(5)\n","df_train_under.filter(F.col('label') == 1).show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+-----+------+\n","|label|node1| node2|\n","+-----+-----+------+\n","|    0|  974|106842|\n","|    0|  981|170237|\n","|    0| 1004|145840|\n","|    0| 1005|232765|\n","|    0| 1005|111798|\n","+-----+-----+------+\n","only showing top 5 rows\n","\n","+-----+-----+-----+\n","|label|node1|node2|\n","+-----+-----+-----+\n","|    1| 1351| 1352|\n","|    1| 2407| 2408|\n","|    1| 2696| 2698|\n","|    1|10044|10052|\n","|    1| 1633|10057|\n","+-----+-----+-----+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X440h4_NldsD","executionInfo":{"status":"ok","timestamp":1623141254513,"user_tz":-120,"elapsed":10722,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"cf9d2797-ae6e-40a4-be3e-2b5e5dfac624"},"source":["df_test_under.filter(F.col('label') == 0).show(5)\n","df_test_under.filter(F.col('label') == 1).show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+-----+------+\n","|label|node1| node2|\n","+-----+-----+------+\n","|    0|  961|218317|\n","|    0|  961|145707|\n","|    0|  970|140082|\n","|    0| 1005|195734|\n","|    0| 1005|191453|\n","+-----+-----+------+\n","only showing top 5 rows\n","\n","+-----+------+------+\n","|label| node1| node2|\n","+-----+------+------+\n","|    1| 78466|145259|\n","|    1|179569|179570|\n","|    1|226715|226716|\n","|    1| 20493|145869|\n","|    1| 28920|165643|\n","+-----+------+------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"koBpOO2ios-F","executionInfo":{"status":"ok","timestamp":1623141254518,"user_tz":-120,"elapsed":33,"user":{"displayName":"Raphaël Azorin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1zZn5o5tIlK5J3P6RdcBMqTBjo6bOd2udjj0W=s64","userId":"12399453063503639914"}},"outputId":"f1de4a42-d7ee-4db1-940a-fb3438225e9a"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UdMDyva_ldsD"},"source":["# Save our DataFrames to CSV files for use in the next notebook\n","\n","save_folder = '/content/gdrive/My Drive/IASD_vacations/IASD_link_prediction/link-prediction/notebooks/data/'\n","\n","df_train_under.write.csv(save_folder + 'df_train_under.csv', mode='overwrite', header=True)\n","df_test_under.write.csv(save_folder + 'df_test_under.csv', mode='overwrite', header=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zP7xBbvTvv6j"},"source":["Please check that both datasets have been written to your Drive at the desired location because we are going to need them later for features engineering."]}]}