{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "py:light,ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "04_IASD_Model_Feature_Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoaqhgJQC_fC"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Install the necessary libraries in your Colab notebook environment and connect to your hosted Neo4J Sandbox."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmHymBmsVYqx",
        "outputId": "fc52d38f-0ab7-4bf5-eecc-34a18d16b9ce"
      },
      "source": [
        "!pip install neo4j pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neo4j\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/22/9b6d28613e8a564b9e82cf3b871b6df1f58a99cf5ac8a100439f9e895b5f/neo4j-4.3.1.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.8MB/s \n",
            "\u001b[?25hCollecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 65kB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from neo4j) (2018.9)\n",
            "Collecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 15.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: neo4j, pyspark\n",
            "  Building wheel for neo4j (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-4.3.1-cp37-none-any.whl size=99332 sha256=eca64fe8bbc4f069d476e2beda94f8ff5d43b9ec6a645d76e52632301812415e\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/13/72/0cc2405898bd9a7baef6512df3abf83873da9ba48c04acc818\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=669cd8720447b8824d9b10064e6764d090ac5b4bfe2d06321cc01522e5cdefff\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built neo4j pyspark\n",
            "Installing collected packages: neo4j, py4j, pyspark\n",
            "Successfully installed neo4j-4.3.1 py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhmJ1QEdC-4e"
      },
      "source": [
        "ip = \"54.84.82.113\"\n",
        "bolt_port = \"7687\"\n",
        "username = \"neo4j\"\n",
        "password = \"balloon-missions-acceleration\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI-e7X2UuOgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328a249d-3895-4d09-b568-829a0e4ef772"
      },
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "driver = GraphDatabase.driver(\"bolt://\" + ip + \":\" + bolt_port, auth=(username, password))\n",
        "\n",
        "print(driver.address) # your-sandbox-ip:your-sandbox-bolt-port"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54.84.82.113:7687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn3TAwvf07Wq"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efG3CA7jrfIO",
        "outputId": "bb9253d9-7638-409a-80c4-2075f3fc9f42"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tImzcaWsrisK"
      },
      "source": [
        "save_folder = '/content/gdrive/My Drive/IASD_vacations/IASD_link_prediction/link-prediction/notebooks/data/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_KGQOcVDyX"
      },
      "source": [
        "# Objective\n",
        "\n",
        "We are going to train a binary classifier to predict wether a link should exist between two *Author* nodes.\n",
        "\n",
        "Each pair of *Author*s will be described with a feature vector and labeled with either 1 (if these two authors have collaborated) or 0 (if they have not)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Fmz1FypJHX"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "Let's generate features for our link prediction classifier. These features will describe a pair of *Author*s by using:\n",
        "- graph topology measures\n",
        "- community detection measures\n",
        "\n",
        "\n",
        "We will identify the nodes by their ID, compute graph measures for these nodes in Neo4J and return a DataFrame with these new features describing each pair of nodes in the train and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz-4iaqtpJHa"
      },
      "source": [
        "Load the CSV files saved in the train/test notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHfF6Fz8pJHa"
      },
      "source": [
        "df_train_under = spark.read.csv(save_folder + 'df_train_under.csv/*.csv', header=True, inferSchema=True).cache()\n",
        "df_test_under = spark.read.csv(save_folder + 'df_test_under.csv/*.csv', header=True, inferSchema=True).cache()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4q1TPS03gQs"
      },
      "source": [
        "Firstly, for Neo4J to be able to manipulate our train and test pairs, we need to feed them as lists of dictionaries. This will enable us to consider each element in this list as a parameter for a Neo4J query. This element's attributes will be accessible to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH6mU1qpkXM0"
      },
      "source": [
        "\n",
        "- Run the following cell to ransform each data frame into a list of dictionaries.\n",
        "\n",
        "```\n",
        "[\n",
        "  {\n",
        "  \"node1\": 15589,\n",
        "  \"node2\": 2567,\n",
        "  \"label\": 1\n",
        "  } ,\n",
        "  ... ,\n",
        "  {\n",
        "  \"node1\": 5466,\n",
        "  \"node2\": 78122,\n",
        "  \"label\": 0\n",
        "  }\n",
        "]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV3ZTmuG4UNd"
      },
      "source": [
        "df_train_under_pairs = [{\"node1\": node1, \"node2\": node2, 'label':label}  for node1, node2, label in df_train_under.select(\"node1\", \"node2\", \"label\").collect()]\n",
        "df_test_under_pairs = [{\"node1\": node1, \"node2\": node2, 'label':label}  for node1, node2, label in df_test_under.select(\"node1\", \"node2\", \"label\").collect()]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDwewWxGpJHb"
      },
      "source": [
        "## Generating graphy features\n",
        "\n",
        "We will start by creating 3 features extracted from the graph topology to describe each pair of nodes: \n",
        "- [common neighbors](https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/common-neighbors/)\n",
        "- [preferential attachment](https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/preferential-attachment/)\n",
        "- [total neighbors](https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/total-neighbors/)\n",
        "\n",
        "We want a final Data Frame with the following structure:\n",
        "\n",
        "| **node1** | **node2** | **label** | **cn** | **pa** | **tn** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epw1BpAtkGHs"
      },
      "source": [
        "By using the *UNWIND* clause, we can manipulate each element in a list as an individual row in Cypher.\n",
        "\n",
        "For example:\n",
        "```\n",
        "# A list of students dictionaries\n",
        "my_list = [{ \"id\": '0001', \"age\": 28}, {\"id\": 0002, \"age\": 35}]\n",
        "\n",
        "# A parameterized query to retrieve each students' name from its attributes\n",
        "query = \n",
        "\"\"\" \n",
        "  UNWIND $list_of_students as student // We use a dollar sign to denote variables\n",
        "    MATCH (s:Student) \n",
        "    WHERE ID(s) = student.id AND s.age = student.age\n",
        "    RETURN s.name\n",
        "\"\"\"\n",
        "\n",
        "with driver.session() as session:\n",
        "    result = session.run(query=query, parameters={\"list_of_students\": my_list})\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAwNbtpw4of_"
      },
      "source": [
        "- Complete the following function to compute the 3 graph measures for pairs of nodes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvCaxBKd-xu2"
      },
      "source": [
        "# Test the individual queries in your Neo4J Browser first.\n",
        "# Then complete the cell to define this function for future execution in Colab.\n",
        "\n",
        "def add_graphy_features(pairs, rel_type):\n",
        "    # Common neighbors\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "        MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "        MATCH (p2) WHERE ID(p2) = ...\n",
        "        RETURN \n",
        "          pair.node1 AS node1,\n",
        "          ... AS node2,\n",
        "          ... AS label,\n",
        "          gds.alpha.linkprediction.commonNeighbors(... , ... , {relationshipQuery: $relType}) AS cn\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"pairs\": pairs, \n",
        "        \"relType\": rel_type\n",
        "        }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_cn = spark.createDataFrame([dict(record) for record in result]) \n",
        "\n",
        "    # Preferential attachment\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "        MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "        MATCH (p2) WHERE ID(p2) = ...\n",
        "        RETURN \n",
        "          pair.node1 AS node1,\n",
        "          ... AS node2,\n",
        "          ... AS label,\n",
        "          gds.alpha.linkprediction.preferentialAttachment(... , ... , {relationshipQuery: $relType}) AS pa\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"pairs\": pairs, \n",
        "        \"relType\": rel_type\n",
        "        }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_pa = spark.createDataFrame([dict(record) for record in result])\n",
        "\n",
        "    # Total neighbors\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "        MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "        MATCH (p2) WHERE ID(p2) = ...\n",
        "        RETURN \n",
        "          pair.node1 AS node1,\n",
        "          ... AS node2,\n",
        "          ... AS label,\n",
        "          gds.alpha.linkprediction.totalNeighbors(... , ... , {relationshipQuery: $relType}) AS tn\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"pairs\": pairs, \n",
        "        \"relType\": rel_type\n",
        "        }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_tn = spark.createDataFrame([dict(record) for record in result])  \n",
        "\n",
        "    # Join the three feature dfs\n",
        "    final_df = ... .join(\n",
        "        ... , on=[\"node1\", \"node2\", \"label\"], how='inner'\n",
        "        ).join(\n",
        "        ... , on=[\"node1\", \"node2\", \"label\"], how='inner'\n",
        "        )\n",
        "\n",
        "    return final_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq097FZPpJHb",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "\n",
        "def add_graphy_features(pairs, rel_type):\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "        MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "        MATCH (p2) WHERE ID(p2) = pair.node2\n",
        "        RETURN \n",
        "          pair.node1 AS node1,\n",
        "          pair.node2 AS node2,\n",
        "          pair.label AS label,\n",
        "          gds.alpha.linkprediction.commonNeighbors(p1, p2, {relationshipQuery: $relType}) AS cn\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"pairs\": pairs, \n",
        "        \"relType\": rel_type\n",
        "        }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_cn = spark.createDataFrame([dict(record) for record in result]) \n",
        "\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "        MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "        MATCH (p2) WHERE ID(p2) = pair.node2\n",
        "        RETURN \n",
        "          pair.node1 AS node1,\n",
        "          pair.node2 AS node2,\n",
        "          pair.label AS label,\n",
        "          gds.alpha.linkprediction.preferentialAttachment(p1, p2, {relationshipQuery: $relType}) AS pa\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"pairs\": pairs, \n",
        "        \"relType\": rel_type\n",
        "        }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_pa = spark.createDataFrame([dict(record) for record in result])\n",
        "\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "        MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "        MATCH (p2) WHERE ID(p2) = pair.node2\n",
        "        RETURN \n",
        "          pair.node1 AS node1,\n",
        "          pair.node2 AS node2,\n",
        "          pair.label AS label,\n",
        "          gds.alpha.linkprediction.totalNeighbors(p1, p2, {relationshipQuery: $relType}) AS tn\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"pairs\": pairs, \n",
        "        \"relType\": rel_type\n",
        "        }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_tn = spark.createDataFrame([dict(record) for record in result])  \n",
        "\n",
        "    final_df = features_cn.join(\n",
        "        features_pa, on=[\"node1\", \"node2\", \"label\"], how='inner'\n",
        "        ).join(\n",
        "        features_tn, on=[\"node1\", \"node2\", \"label\"], how='inner'\n",
        "        )\n",
        "\n",
        "    return final_df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygZi81CRpJHb"
      },
      "source": [
        "Let's apply the function to our training DataFrame and do a quick sanity check of the number of resulting rows (that should be the same).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgE3340dAlmk"
      },
      "source": [
        "print('Counts before applying graph features engineering: ')\n",
        "print(df_train_under.count())\n",
        "print(df_test_under.count())\n",
        "\n",
        "df_train_under_graph_f = add_graphy_features(df_train_under_pairs, \"CO_AUTHOR_EARLY\")\n",
        "df_test_under_graph_f = add_graphy_features(df_test_under_pairs, \"CO_AUTHOR_LATE\")\n",
        "\n",
        "print('Counts after applying graph features engineering: ')\n",
        "print(df_train_under_graph_f.count())\n",
        "print(df_test_under_graph_f.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OC8AqJoSuOs"
      },
      "source": [
        "Let's see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbnB6fsPSbvt"
      },
      "source": [
        "df_train_under_graph_f.filter(F.col('label') == 0).show(5)\n",
        "df_test_under_graph_f.filter(F.col('label') == 1).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu5X15uIpJHc"
      },
      "source": [
        "## Generating community features\n",
        "\n",
        "Community detection algorithms evaluate how a group is clustered or partitioned. Nodes are considered more similar to nodes that fall in their community than to nodes in other communities.\n",
        "\n",
        "We will extract one feature based on a community detection algorithm:\n",
        "\n",
        "- [Louvain community](https://neo4j.com/docs/graph-data-science/current/algorithms/louvain/)\n",
        "\n",
        "The Louvain algorithm returns intermediate communities found in the graph. We will add a property to each node containing the first community that the algorithm found for this specific node. Thus, it will constitue a categorical feature : the first community to which each node belongs. As we are considering pairs of nodes, we will derive a *'same_community_louvain'* binary feature (True or False) to further describe each pair of nodes. \n",
        "\n",
        "Note that we need to restrict the execution of the Louvain community detection algorithm to the train and test subgraphs separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn7T8-_O48Rs"
      },
      "source": [
        "- Set a property on each node in the *CO_AUTHOR_EARLY* subgraph, containing the first community ID that the Louvain algorithm found for this node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34hmbUehpJHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd7b6f49-8b5a-4525-c6f3-0e41171b0337"
      },
      "source": [
        "query = \"\"\"\n",
        "  CALL gds.louvain.stream({\n",
        "    nodeProjection: 'Author',\n",
        "    relationshipProjection: {\n",
        "      CO_AUTHOR_EARLY: {\n",
        "        type: 'CO_AUTHOR_EARLY',\n",
        "        orientation: 'UNDIRECTED'\n",
        "      }\n",
        "    },\n",
        "    includeIntermediateCommunities: true\n",
        "  })\n",
        "  YIELD nodeId, communityId, intermediateCommunityIds\n",
        "  WITH gds.util.asNode(nodeId) AS node, intermediateCommunityIds[0] AS smallestCommunity\n",
        "  SET node.louvainTrain = smallestCommunity;\n",
        "  \"\"\"\n",
        "\n",
        "with driver.session() as session:\n",
        "    display(session.run(query).consume().counters)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'properties_set': 80299}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlfAQ55d5UjJ"
      },
      "source": [
        "- Similarly, set a property on each node in the *CO_AUTHOR_LATE* subgraph, containing the first community ID that the Louvain algorithm found for this node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwPYWSOdpJHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e14d4a3-5b76-4c86-dabf-a5cbaee1ca44"
      },
      "source": [
        "query = \"\"\"\n",
        "  CALL gds.louvain.stream({\n",
        "    nodeProjection: 'Author',\n",
        "    relationshipProjection: {\n",
        "      CO_AUTHOR_LATE: {\n",
        "        type: 'CO_AUTHOR_LATE',\n",
        "        orientation: 'UNDIRECTED'\n",
        "      }\n",
        "    },\n",
        "    includeIntermediateCommunities: true\n",
        "  })\n",
        "  YIELD nodeId, communityId, intermediateCommunityIds\n",
        "  WITH gds.util.asNode(nodeId) AS node, intermediateCommunityIds[0] AS smallestCommunity\n",
        "  SET node.louvainTest = smallestCommunity;\n",
        "  \"\"\"\n",
        "\n",
        "with driver.session() as session:\n",
        "    display(session.run(query).consume().counters)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'properties_set': 80299}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liWanN3o6LK3"
      },
      "source": [
        "- Now, each node in our graph contains 2 new properties. What are the names of these properties? \n",
        "\n",
        "\n",
        "**Hint:** Feel free to use the Neo4J Browser if it doesn't look intuitive from the code blocks above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iUwKSPo6xpJ",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "\n",
        "\"\"\"\n",
        "- louvainTrain: the ID of the first community found for the node when using CO_AUTHOR_EARLY edges only\n",
        "- louvainTest:  the ID of the first community found for the node when using CO_AUTHOR_LATE edges only\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbG0VSN36YJf"
      },
      "source": [
        "Let's now build a derived feature to express for each pair wether the nodes belong to the same Louvain community or not.\n",
        "\n",
        "- Complete the function below to create this derived feature for each pair of nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwKKEglh9B7J"
      },
      "source": [
        "def add_community_feature(pairs, louvain_prop):\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "      MATCH (p1) WHERE pair.node1\n",
        "      MATCH (p2) WHERE ...\n",
        "      RETURN \n",
        "        pair.node1 AS node1,\n",
        "        ... AS node2,\n",
        "        ... AS label,\n",
        "        gds.alpha.linkprediction.sameCommunity(... , ... , ...) AS sl\n",
        "    \"\"\"\n",
        "    params = {\n",
        "    \"pairs\": pairs,\n",
        "    \"louvainProp\": louvain_prop\n",
        "    }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_sl = spark.createDataFrame([dict(record) for record in result])\n",
        "    \n",
        "    return features_sl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsdrnftopJHc",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "\n",
        "def add_community_feature(pairs, louvain_prop):\n",
        "    query = \"\"\"\n",
        "      UNWIND $pairs AS pair\n",
        "      MATCH (p1) WHERE ID(p1) = pair.node1\n",
        "      MATCH (p2) WHERE ID(p2) = pair.node2\n",
        "      RETURN \n",
        "        pair.node1 AS node1,\n",
        "        pair.node2 AS node2,\n",
        "        pair.label AS label,\n",
        "        gds.alpha.linkprediction.sameCommunity(p1, p2, $louvainProp) AS sl\n",
        "    \"\"\"\n",
        "    params = {\n",
        "    \"pairs\": pairs,\n",
        "    \"louvainProp\": louvain_prop\n",
        "    }\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query, params)\n",
        "        features_sl = spark.createDataFrame([dict(record) for record in result])\n",
        "    \n",
        "    return features_sl"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n31STYVnB2Qu"
      },
      "source": [
        "Let's apply the function to our training DataFrame and do a quick sanity check of the number of resulting rows (that should be the same)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdF3yinbpJHd"
      },
      "source": [
        "print('Counts before applying community feature engineering: ')\n",
        "print(df_train_under.count())\n",
        "print(df_test_under.count())\n",
        "\n",
        "df_train_under_community_f = add_community_feature(df_train_under_pairs, \"louvainTrain\")\n",
        "df_test_under_community_f = add_community_feature(df_test_under_pairs, \"louvainTest\")\n",
        "\n",
        "print('Counts after applying community feature engineering: ')\n",
        "print(df_train_under_community_f.count())\n",
        "print(df_test_under_community_f.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTd8AO8wYIH6"
      },
      "source": [
        "Let's see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjFh7FdqYHcg"
      },
      "source": [
        "df_train_under_community_f.filter(F.col('label') == 0).show(5)\n",
        "df_test_under_community_f.filter(F.col('label') == 1).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg6shFo0YTHU"
      },
      "source": [
        "# Save train and test DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUrJkmFTAZaH"
      },
      "source": [
        "- Join the graph topology features df with the community feature df."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIz27oaoAfin"
      },
      "source": [
        "print('Counts before joining df: ')\n",
        "print(df_train_under_graph_f.count())\n",
        "print(df_train_under_community_f.count())\n",
        "\n",
        "print(df_test_under_graph_f.count())\n",
        "print(df_test_under_community_f.count())\n",
        "\n",
        "df_train_under = df_train_under_graph_f.join(df_train_under_community_f, on=['node1', 'node2', 'label'], how='inner')\n",
        "df_test_under = df_test_under_graph_f.join(df_test_under_community_f, on=['node1', 'node2', 'label'], how='inner')\n",
        "\n",
        "print('Counts after joining df: ')\n",
        "print(df_train_under.count())\n",
        "print(df_test_under.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCJ2oY5k_rHy"
      },
      "source": [
        "Save our final train and test DataFrames to CSV files for use in the next notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO8I7WZKpJHd"
      },
      "source": [
        "df_train_under.write.csv(save_folder +  \"df_train_under_all.csv\", mode='overwrite', header=True)\n",
        "df_test_under.write.csv(save_folder + \"df_test_under_all.csv\", mode='overwrite', header=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgwF_gOeAPQZ"
      },
      "source": [
        "Please check that both datasets have been written to your Drive at the desired location because we are going to need them later for model training and testing."
      ]
    }
  ]
}